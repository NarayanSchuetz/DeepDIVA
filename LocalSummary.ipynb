{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentArgs(object):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        self._args = args\n",
    "        \n",
    "    @property\n",
    "    def experiment_name(self):\n",
    "        return self._args[\"experiment_name\"]\n",
    "    \n",
    "    @property\n",
    "    def dataset_name(self):\n",
    "        return self._extract_dataset_name()\n",
    "    \n",
    "    @property\n",
    "    def model_name(self):\n",
    "        return self._args[\"model_name\"]\n",
    "        \n",
    "    def _extract_dataset_name(self):\n",
    "        split = self._args[\"dataset_folder\"].split(\"/\")\n",
    "        for i in np.array(range(1, len(split)+1))*-1:\n",
    "            ds_name = split[i]\n",
    "            if ds_name != \"\":\n",
    "                return ds_name\n",
    "        else:\n",
    "            return \"\"\n",
    "        \n",
    "    @classmethod\n",
    "    def from_json_file(cls, path):\n",
    "        with open(path, \"r\") as f:\n",
    "            args = json.load(f)\n",
    "            return cls(args)\n",
    "        \n",
    "        \n",
    "class ExperimentLogs(object):\n",
    "    \n",
    "    _GRAND_MEAN_STD_PATTERN = re.compile(r\"Multi-run values for test-mean:(\\d+\\.\\d+) test-std: (\\d+\\.\\d+)\")\n",
    "    _TRAINABLE_PARAMS_PATTERN = re.compile(r\"Number of trainable model parameters: (\\d+)\")\n",
    "    _TOTAL_PARAMS_PATTERN = re.compile(r\"Number of total model parameters: (\\d+)\")\n",
    "    _NUMBER_REPETITIONS_PATTERN = re.compile(r\"Multi-Run: \\d+ of (\\d+)\")\n",
    "    \n",
    "    def __init__(self, log):\n",
    "        self._log = log\n",
    "        \n",
    "        self._test_grandmean = None\n",
    "        self._test_std = None\n",
    "        self._trainable_model_params = None\n",
    "        self._total_model_params = None\n",
    "        self._number_repetitions = None\n",
    "        \n",
    "    @property\n",
    "    def test_accuracy_grandmean(self):\n",
    "        if self._test_grandmean is None:\n",
    "            self._parse_grandmean_and_std()\n",
    "        return self._test_grandmean\n",
    "    \n",
    "    @property\n",
    "    def test_accuracy_std(self):\n",
    "        if self._test_grandmean is None:\n",
    "            self._parse_grandmean_and_std()\n",
    "        return self._test_std\n",
    "    \n",
    "    @property\n",
    "    def trainable_model_params(self):\n",
    "        if self._trainable_model_params is None:\n",
    "            self._parse_number_parameters()\n",
    "        return self._trainable_model_params\n",
    "    \n",
    "    @property\n",
    "    def total_model_params(self):\n",
    "        if self._total_model_params is None:\n",
    "            self._parse_number_parameters()\n",
    "        return self._total_model_params\n",
    "    \n",
    "    @property\n",
    "    def number_repetitions(self):\n",
    "        if self._number_repetitions is None:\n",
    "            self._parse_number_repetitions()\n",
    "        return self._number_repetitions\n",
    "\n",
    "    def _parse_grandmean_and_std(self):\n",
    "        try:\n",
    "            match = re.findall(self._GRAND_MEAN_STD_PATTERN, self._log)[0]\n",
    "            self._test_grandmean = float(match[0])\n",
    "            self._test_std = float(match[1])\n",
    "        except Exception as e:\n",
    "            self._test_grandmean = np.nan\n",
    "            self._test_std = np.nan\n",
    "            warnings.warn(\"Log files does not seem complete, it might be corrupted or otherwise missing results: %s\"\n",
    "                         % str(e))\n",
    "        \n",
    "    def _parse_number_parameters(self):\n",
    "        try:\n",
    "            match_trainable = re.findall(self._TRAINABLE_PARAMS_PATTERN, self._log)\n",
    "            match_total = re.findall(self._TOTAL_PARAMS_PATTERN, self._log)\n",
    "            self._trainable_model_params = int(match_trainable[0])\n",
    "            self._total_model_params = int(match_trainable[0])\n",
    "        except Exception as e:\n",
    "            self._trainable_model_params = np.nan\n",
    "            self._total_model_params = np.nan\n",
    "            warnings.warn(\"Log files does not seem complete, it might be corrupted or otherwise missing results: %s\"\n",
    "                         % str(e))\n",
    "            \n",
    "    def _parse_number_repetitions(self):\n",
    "        try:\n",
    "            match_number_repetitions = re.findall(self._NUMBER_REPETITIONS_PATTERN, self._log)\n",
    "            self._number_repetitions = int(match_number_repetitions[0])\n",
    "        except:\n",
    "            self._number_repetitions = np.nan\n",
    "            warnings.warn(\"Log files does not seem complete, it might be corrupted or otherwise missing results: %s\"\n",
    "                         % str(e))\n",
    "            \n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def from_log_file(cls, path):\n",
    "        with open(path, \"r\") as f:\n",
    "            return cls(f.read())\n",
    "        \n",
    "class Experiment(object):\n",
    "    \n",
    "    def __init__(self, experiment_logs, experiment_args, path, name=None):\n",
    "        self.experiment_logs = experiment_logs\n",
    "        self.experiment_args = experiment_args\n",
    "        self.path = path\n",
    "        self._name = name\n",
    "        \n",
    "    @property\n",
    "    def experiment_name(self):\n",
    "        return self._name if self._name is not None else self.experiment_args.experiment_name\n",
    "    \n",
    "    @property\n",
    "    def dataset_name(self):\n",
    "        return self.experiment_args.dataset_name\n",
    "    \n",
    "    @property\n",
    "    def model_name(self):\n",
    "        return self.experiment_args.model_name\n",
    "    \n",
    "    @property\n",
    "    def test_accuracy(self):\n",
    "        return self.experiment_logs.test_accuracy_grandmean\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return self.test_accuracy < other.test_accuracy\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Experiment Name: %s\\nModel Name: %s\\nDataset Name: %s\\nTest Accuracy: %f\\n\" % (\n",
    "            self.experiment_name, self.model_name, self.dataset_name, self.test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = []\n",
    "for root, dirs, files in os.walk('log/SpectralModelEvaluation'):\n",
    "    if 'logs.txt' in files:\n",
    "        args = ExperimentArgs.from_json_file(os.path.join(root, \"args.txt\"))\n",
    "        logs = ExperimentLogs.from_log_file(os.path.join(root, \"logs.txt\"))\n",
    "        experiment = Experiment(logs, args, root)\n",
    "        experiments.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "for experiment in experiments:\n",
    "    name = experiment.model_name\n",
    "    dataset = experiment.dataset_name\n",
    "    mean = experiment.experiment_logs.test_accuracy_grandmean\n",
    "    std = experiment.experiment_logs.test_accuracy_std\n",
    "    trainable_parameters = experiment.experiment_logs.trainable_model_params\n",
    "    total_parameters = experiment.experiment_logs.total_model_params\n",
    "    number_repetitions = experiment.experiment_logs.number_repetitions\n",
    "    \n",
    "    record = (name, dataset, mean, std, std/np.sqrt(10), trainable_parameters, total_parameters, number_repetitions)\n",
    "    records.append(record)\n",
    "    \n",
    "experiment_df = pd.DataFrame.from_records(records, \n",
    "                                          columns=[\n",
    "                                              \"ModelName\",\n",
    "                                              \"Dataset\",\n",
    "                                              \"MeanTestAccuracy\", \n",
    "                                              \"TestStandardDeviation\", \n",
    "                                              \"TestStandardError\", \n",
    "                                              \"TrainableParameters\", \n",
    "                                              \"TotalParameters\",\n",
    "                                              \"NumberRepetitions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df.to_csv(\"../summary_experiments2d.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepdiva]",
   "language": "python",
   "name": "conda-env-deepdiva-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
